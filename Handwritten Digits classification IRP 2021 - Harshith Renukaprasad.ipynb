{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How increasing the amount of training data for a machine learning algorithm that recognizes handwritten digits increases the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Harshith Renukaprasad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the scientific study of algorithms ancd statistical models that computers use to perform a specific task without a human explicitly asking it to do so. Practically, this means giving the computer a training set of images for the computer to learn the difference between a 3 and an 8 for example, and then give the image an image of a 3 it has not seen before and predict what number it is based on what it has learnt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Goal of the project is to progressively increase the amount of training images available to the computer from  100 to 50,000 and document the increased accuracy and decreased loss of the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materials required for the project:\n",
    "\n",
    "Disclaimer: Most of the materials included are digital.\n",
    "\n",
    "Another Disclaimer: This project was done on Visual Studio Code, but is submitted/presented in Jupyter notebook for the users ease in readablity.\n",
    "\n",
    "1. The first and foremost thing required for this project is a computer with  [Visual Studio Code ](https://code.visualstudio.com/download) (200  megabytes, Windows 64 bit) installed. Note that the computer needs to  have atleast 1 gigabyte of ram and 5 gigabytes of storage available. The computer, very importantly, also requires a stable internet connection. Finally, [Python 3.7.6](https://www.python.org/ftp/python/3.7.6/python-3.7.6-amd64.exe) (downloaded, and added to path throught the installer)needs to be installed on the system\n",
    "\n",
    "\n",
    "2. A google account to login to the websites [Hackerrank](https://www.hackerrank.com/), and [Youtube](https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g). Hackerrank is a very popular website used to practice python, since this whole project is written using the language Python (version 3.7.6). Youtube is required to actually learn the language and the hyperlink on the word \"Youtube\" earlier links to a channel called Corey Schafer, from which all the learning for python, and its various packages were done.\n",
    "\n",
    "\n",
    "3. Speaking of packages, the there are only 4 packages required for this project which are Keras, Sklearn, pandas, matplotlib and finally tensorflow. All of these can be installed by opening up Visual Studio Code, opening a new terminal by clicking on terminal on the top taskbar, and typing in \"pip install keras\", wait for it to install, \"pip install sklearn\", waiting for Sklearn to install, and finally \"pip install tensorflow\", and allow tensorflow(make sure version 2.1.0 is installing) to finish installing. Pandas and Matplotlib can be installed using the same \"pip install \" command. Do the same with  Note that the full extent of some packages are not used, rather a parts of it or \"Modules\" are used.\n",
    "\n",
    "\n",
    "4. A lot of time, around 100 to 120 hours of work are required to finish the project. 60 hours are required to learn python, 20 to learn about machine learning and another 20 hours to actually code. This may take longer or shorter depending on the amount of coding/python known. \n",
    "\n",
    "\n",
    "5. A website known as [Markdown Tables gererator](https://www.tablesgenerator.com/markdown_tables) which allows for the data table of this project to be created and embedded into the markdown cells(text cells instead of code cells) with ease.\n",
    "\n",
    "\n",
    "6. Also, this project has a link further on to the mnist database of handwritten numbers, but that is just for ease of access, as keras has a module through which mnist can be imported.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safety precautions or Collection Concerns: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the final iteration of the project is collected using an algorithm in the algorithm. Akin to a data collection journal, the program has an initialized dictionary, or a set of values that are updated for each data set(For example 1000 images, or 2000 images  for training). Each data set is like a different entry into the dictionary, with its own results. The algorithm/formula uses a for loop to test each data set and at the end, the results are appended into the dictionary. This ensures that no data is lost, and everything is collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!: The procedure details the actual coding part of the project, assuming that the reader already knows python and has installed all of the packages necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: import all the necessary packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem like a lot, but it's actually not. Using just 3 packages, keras(Tensorflow is used by keras, and so it is neccessary to install tensorflow) pandas, sklearn, all the modules and everything needed for this project is imported. The main things to take not of from keras are mnist, and conv2d. [Mnist](http://yann.lecun.com/exdb/mnist/) is the database from which the labeled numerical digits are imported. Conv2d is just a module which helps to build a convolutional neural network(Refer to the image below). From sklearn, the main module to be noted is train_test_split, which allows us to vary the amount of training data. Pandas and matplotlib are used for data plotting and making a data table internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of a convolutional neural network:\n",
    "\n",
    "![Image of a convolutional neural network](https://cdn-images-1.medium.com/fit/t/1600/480/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Write a function to load the MNIST database into training and testing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(test_size_per):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train, y_train, test_size=test_size_per)\n",
    "    print(x_train.shape)\n",
    "    return x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get into the actual coding bit of the project. This is one of 3 functions in this project. The name of it being \"load_minst_data\". This does exactly what the name suggests and loads the variables with training and test data. The variables being x_train, y_train, x_test, and y_test. x and y are not coordinates, but rather x is the input and y is the output. The training variables, x_train, and  y_train are where all the training images that the computer uses to learn are kept. While x_test, and y_test are where the testing images are kept. These images are what the computer tests itself on to refine the algorithm. There are 60,000 images in the mnist database and by default, 50,000 are kept for training and the remaining 10,000 are used as testing images. on the 4th and 5th lines of the cell, the function train_test_split() is used to split the data into training and testing images by taking a percentage value which is then applied to the database and made into the test set. For example, if the percentage is given as 0.5 or 50%, 30,000 images would be used as testing and 30,000 for training. This is not a set value however, there is a parameter known as test_size_per which allows the percentage to be changed every time the function is called upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a sample image from the mnist database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Handwritten 8 in the MNIST database](https://miro.medium.com/max/490/1*nlfLUgHUEj5vW7WVJpxY-g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the image is graphed and has a size of 28  x 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write a function to reshape, and prepare all the images uniformly for input into the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(x_train, y_train, x_test, y_test):\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important parts of the whole algorithm, second only to the actual machine learning part itself. All the images are now reshaped into 28 x 28, just in case some of them were not that size. Furthermore, this uniformity allows the computer to look at all of the images consistently the same. Furthermore, all the images are shuffled, so that certain patterns and exclusions of digits do not occur, therefore removing the chance that the computer's \"learning\" is skewed. In summation, this function allows for the uniformity of the images so that the computer sees every image equally and is not skewed by unnatural patterns such as the abscence of a certain digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Writing a function to setup the model, train the model, and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(x_train, y_train, x_test, y_test ):\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 10\n",
    "\n",
    "    input_shape = (28, 28, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "    print(\"The model has successfully trained\")\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "\n",
    "    model.save('mnist.h5')\n",
    "    print(\"Saving the model as mnist.h5\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is what sets up a model(convolutional neural network), and trains it with the training set. This is still a function, and is not executing anything, so no output is given. The way a machine learning model learns/trains itself is that it uses differential calculus. Bascially, an inverse parabola is graphed internally, called the gradient descent that allows the computer to see where it is. The x axis is the answer that it is guessing. For example, an eight is given as an input, and the computer guesses that it's a 5. Now, that is the wrong answer, but atleast the computer now knows that the eight is not a 5. The y axis is the cost. Basically the higher the cost, the farther the model is from the answer. Graphing this would show an inverse parabola, where the answer(the number eight in the example) on the x axis, and the lowest value on the y axis(the cost) are located. The end goal of the model is to reach that low point. once it does, and it gets a problem correct, it moves on and uses the same formula that it used for the previous one. Basically, the computer at first uses random values, and then narrows it down to the correct value, and uses the formula that got it the correct values. This is all calculated in a matter of mere seconds though. This is why a larger training set, like common sense would tell us, makes the computer more accurate in its learning and testing. Below is an image of the gradient descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Gradient Descent](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Executing the functions in a for loop, so that each iteration, a bigger training set is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(100, 28, 28)\n",
      "Testing with train set size of:  (100, 28, 28)\n",
      "x_train shape: (100, 28, 28, 1)\n",
      "100 train samples\n",
      "59900 test samples\n",
      "Train on 100 samples, validate on 59900 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 2.3117 - accuracy: 0.0900 - val_loss: 2.2862 - val_accuracy: 0.2059\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 2.2679 - accuracy: 0.2100 - val_loss: 2.2642 - val_accuracy: 0.2679\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 2.2501 - accuracy: 0.1600 - val_loss: 2.2493 - val_accuracy: 0.1748\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 2.2122 - accuracy: 0.2200 - val_loss: 2.2274 - val_accuracy: 0.1498\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 2.1858 - accuracy: 0.2800 - val_loss: 2.2027 - val_accuracy: 0.1829\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 2.1288 - accuracy: 0.2400 - val_loss: 2.1618 - val_accuracy: 0.2567\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 2.1128 - accuracy: 0.2600 - val_loss: 2.1396 - val_accuracy: 0.3057\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 2.0578 - accuracy: 0.2600 - val_loss: 2.1106 - val_accuracy: 0.1816\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 2.0516 - accuracy: 0.2400 - val_loss: 1.9946 - val_accuracy: 0.4640\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 1.9776 - accuracy: 0.3200 - val_loss: 1.9474 - val_accuracy: 0.4136\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9966666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(200, 28, 28)\n",
      "Testing with train set size of:  (200, 28, 28)\n",
      "x_train shape: (200, 28, 28, 1)\n",
      "200 train samples\n",
      "59800 test samples\n",
      "Train on 200 samples, validate on 59800 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 2.3065 - accuracy: 0.0800 - val_loss: 2.2781 - val_accuracy: 0.1150\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 2.2574 - accuracy: 0.1500 - val_loss: 2.2518 - val_accuracy: 0.1520\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 11s 57ms/step - loss: 2.2570 - accuracy: 0.0950 - val_loss: 2.2066 - val_accuracy: 0.1192\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 2.2060 - accuracy: 0.1650 - val_loss: 2.1518 - val_accuracy: 0.4252\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 2.1723 - accuracy: 0.1850 - val_loss: 2.1369 - val_accuracy: 0.2132\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 2.1272 - accuracy: 0.2550 - val_loss: 2.0190 - val_accuracy: 0.3236\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 2.0867 - accuracy: 0.2700 - val_loss: 2.0356 - val_accuracy: 0.3126\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 2.0230 - accuracy: 0.3000 - val_loss: 1.7765 - val_accuracy: 0.5902\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 1.8218 - accuracy: 0.3850 - val_loss: 1.7845 - val_accuracy: 0.3602\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 1.8762 - accuracy: 0.3400 - val_loss: 1.7486 - val_accuracy: 0.5124\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.995\n",
      "(60000, 28, 28) (60000,)\n",
      "(300, 28, 28)\n",
      "Testing with train set size of:  (300, 28, 28)\n",
      "x_train shape: (300, 28, 28, 1)\n",
      "300 train samples\n",
      "59700 test samples\n",
      "Train on 300 samples, validate on 59700 samples\n",
      "Epoch 1/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 2.3023 - accuracy: 0.1267 - val_loss: 2.2708 - val_accuracy: 0.1019\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 2.2497 - accuracy: 0.1800 - val_loss: 2.2178 - val_accuracy: 0.1244\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 2.1949 - accuracy: 0.2100 - val_loss: 2.1362 - val_accuracy: 0.2035\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 10s 35ms/step - loss: 2.1263 - accuracy: 0.2233 - val_loss: 2.0077 - val_accuracy: 0.5505\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 11s 35ms/step - loss: 2.0001 - accuracy: 0.2967 - val_loss: 1.9212 - val_accuracy: 0.2624\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 13s 42ms/step - loss: 1.9954 - accuracy: 0.2867 - val_loss: 1.6869 - val_accuracy: 0.6276\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 1.7400 - accuracy: 0.4333 - val_loss: 1.6392 - val_accuracy: 0.4327\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 11s 36ms/step - loss: 1.7066 - accuracy: 0.4333 - val_loss: 1.2951 - val_accuracy: 0.6281\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 12s 40ms/step - loss: 1.5044 - accuracy: 0.5000 - val_loss: 1.2265 - val_accuracy: 0.6582\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 11s 37ms/step - loss: 1.3611 - accuracy: 0.5300 - val_loss: 0.9859 - val_accuracy: 0.7883\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9933333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(400, 28, 28)\n",
      "Testing with train set size of:  (400, 28, 28)\n",
      "x_train shape: (400, 28, 28, 1)\n",
      "400 train samples\n",
      "59600 test samples\n",
      "Train on 400 samples, validate on 59600 samples\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 2.2979 - accuracy: 0.1525 - val_loss: 2.2714 - val_accuracy: 0.1775\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 2.2163 - accuracy: 0.1700 - val_loss: 2.1560 - val_accuracy: 0.3127\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 13s 32ms/step - loss: 2.1427 - accuracy: 0.2200 - val_loss: 2.1202 - val_accuracy: 0.1869\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 2.0840 - accuracy: 0.2425 - val_loss: 1.8108 - val_accuracy: 0.5712\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 1.8986 - accuracy: 0.3425 - val_loss: 1.7931 - val_accuracy: 0.4050\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 1.8151 - accuracy: 0.3625 - val_loss: 1.5609 - val_accuracy: 0.5302\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 11s 26ms/step - loss: 1.6738 - accuracy: 0.4300 - val_loss: 1.3448 - val_accuracy: 0.5663\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 1.4808 - accuracy: 0.5125 - val_loss: 0.9635 - val_accuracy: 0.7289\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 11s 27ms/step - loss: 1.2246 - accuracy: 0.6075 - val_loss: 1.8114 - val_accuracy: 0.3914\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 11s 26ms/step - loss: 1.6187 - accuracy: 0.4925 - val_loss: 0.8299 - val_accuracy: 0.7593\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9916666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(500, 28, 28)\n",
      "Testing with train set size of:  (500, 28, 28)\n",
      "x_train shape: (500, 28, 28, 1)\n",
      "500 train samples\n",
      "59500 test samples\n",
      "Train on 500 samples, validate on 59500 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 11s 21ms/step - loss: 2.2915 - accuracy: 0.1520 - val_loss: 2.2344 - val_accuracy: 0.2735\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 15s 29ms/step - loss: 2.2184 - accuracy: 0.2100 - val_loss: 2.1304 - val_accuracy: 0.2635\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 2.0686 - accuracy: 0.3140 - val_loss: 1.9462 - val_accuracy: 0.3147\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 11s 21ms/step - loss: 2.0143 - accuracy: 0.2840 - val_loss: 1.6752 - val_accuracy: 0.5093\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 1.8996 - accuracy: 0.3340 - val_loss: 1.5989 - val_accuracy: 0.5398\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 1.6635 - accuracy: 0.4560 - val_loss: 1.1175 - val_accuracy: 0.7032\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 1.4251 - accuracy: 0.5040 - val_loss: 1.2945 - val_accuracy: 0.5572\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 1.3066 - accuracy: 0.5520 - val_loss: 1.0419 - val_accuracy: 0.6540\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 1.0993 - accuracy: 0.6320 - val_loss: 0.8825 - val_accuracy: 0.6981\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 11s 21ms/step - loss: 1.1477 - accuracy: 0.5940 - val_loss: 0.6113 - val_accuracy: 0.8489\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.99\n",
      "(60000, 28, 28) (60000,)\n",
      "(600, 28, 28)\n",
      "Testing with train set size of:  (600, 28, 28)\n",
      "x_train shape: (600, 28, 28, 1)\n",
      "600 train samples\n",
      "59400 test samples\n",
      "Train on 600 samples, validate on 59400 samples\n",
      "Epoch 1/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 2.2921 - accuracy: 0.1133 - val_loss: 2.2331 - val_accuracy: 0.2438\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 2.2131 - accuracy: 0.1750 - val_loss: 2.1035 - val_accuracy: 0.3580\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 2.1091 - accuracy: 0.2850 - val_loss: 1.8949 - val_accuracy: 0.5338\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 1.9377 - accuracy: 0.3150 - val_loss: 1.7653 - val_accuracy: 0.3502\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 1.7313 - accuracy: 0.3983 - val_loss: 1.2287 - val_accuracy: 0.6453\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 1.4136 - accuracy: 0.5217 - val_loss: 0.8218 - val_accuracy: 0.7969\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 1.2522 - accuracy: 0.5917 - val_loss: 0.7405 - val_accuracy: 0.8733\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9721 - accuracy: 0.6750 - val_loss: 0.5883 - val_accuracy: 0.8552\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 1.0473 - accuracy: 0.6417 - val_loss: 0.5580 - val_accuracy: 0.8728\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 11s 18ms/step - loss: 0.7741 - accuracy: 0.7350 - val_loss: 0.3970 - val_accuracy: 0.9013\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9883333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(700, 28, 28)\n",
      "Testing with train set size of:  (700, 28, 28)\n",
      "x_train shape: (700, 28, 28, 1)\n",
      "700 train samples\n",
      "59300 test samples\n",
      "Train on 700 samples, validate on 59300 samples\n",
      "Epoch 1/10\n",
      "700/700 [==============================] - 11s 16ms/step - loss: 2.2778 - accuracy: 0.1243 - val_loss: 2.1848 - val_accuracy: 0.4624\n",
      "Epoch 2/10\n",
      "700/700 [==============================] - 11s 16ms/step - loss: 2.1219 - accuracy: 0.2529 - val_loss: 1.9712 - val_accuracy: 0.3544\n",
      "Epoch 3/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 1.9779 - accuracy: 0.2800 - val_loss: 1.6921 - val_accuracy: 0.4866\n",
      "Epoch 4/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 1.7726 - accuracy: 0.3886 - val_loss: 1.4623 - val_accuracy: 0.6864\n",
      "Epoch 5/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 1.3912 - accuracy: 0.5371 - val_loss: 0.8085 - val_accuracy: 0.7360\n",
      "Epoch 6/10\n",
      "700/700 [==============================] - 13s 18ms/step - loss: 1.1790 - accuracy: 0.6014 - val_loss: 0.7642 - val_accuracy: 0.7909\n",
      "Epoch 7/10\n",
      "700/700 [==============================] - 12s 17ms/step - loss: 0.9696 - accuracy: 0.6900 - val_loss: 0.4888 - val_accuracy: 0.8726\n",
      "Epoch 8/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 0.7049 - accuracy: 0.7771 - val_loss: 0.4045 - val_accuracy: 0.8720\n",
      "Epoch 9/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 0.6965 - accuracy: 0.7614 - val_loss: 0.5663 - val_accuracy: 0.8145\n",
      "Epoch 10/10\n",
      "700/700 [==============================] - 11s 15ms/step - loss: 0.6449 - accuracy: 0.8114 - val_loss: 0.3194 - val_accuracy: 0.9036\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9866666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(800, 28, 28)\n",
      "Testing with train set size of:  (800, 28, 28)\n",
      "x_train shape: (800, 28, 28, 1)\n",
      "800 train samples\n",
      "59200 test samples\n",
      "Train on 800 samples, validate on 59200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 2.2741 - accuracy: 0.1412 - val_loss: 2.2143 - val_accuracy: 0.0990\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 2.1362 - accuracy: 0.2375 - val_loss: 1.9732 - val_accuracy: 0.3168\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 13s 16ms/step - loss: 1.9027 - accuracy: 0.3487 - val_loss: 1.6722 - val_accuracy: 0.4162\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 1.5240 - accuracy: 0.4988 - val_loss: 1.2136 - val_accuracy: 0.5897\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 1.2767 - accuracy: 0.5800 - val_loss: 0.8748 - val_accuracy: 0.7820\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.9594 - accuracy: 0.6900 - val_loss: 0.5812 - val_accuracy: 0.8404\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.8518 - accuracy: 0.7212 - val_loss: 0.6028 - val_accuracy: 0.8376\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.7121 - accuracy: 0.7775 - val_loss: 0.4995 - val_accuracy: 0.8399\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.6012 - accuracy: 0.8125 - val_loss: 0.3811 - val_accuracy: 0.8890\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 11s 14ms/step - loss: 0.5690 - accuracy: 0.8313 - val_loss: 0.3718 - val_accuracy: 0.8869\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.985\n",
      "(60000, 28, 28) (60000,)\n",
      "(900, 28, 28)\n",
      "Testing with train set size of:  (900, 28, 28)\n",
      "x_train shape: (900, 28, 28, 1)\n",
      "900 train samples\n",
      "59100 test samples\n",
      "Train on 900 samples, validate on 59100 samples\n",
      "Epoch 1/10\n",
      "900/900 [==============================] - 13s 15ms/step - loss: 2.2669 - accuracy: 0.1522 - val_loss: 2.2453 - val_accuracy: 0.0974\n",
      "Epoch 2/10\n",
      "900/900 [==============================] - 18s 20ms/step - loss: 2.1561 - accuracy: 0.2333 - val_loss: 2.1198 - val_accuracy: 0.2436\n",
      "Epoch 3/10\n",
      "900/900 [==============================] - 14s 16ms/step - loss: 1.9353 - accuracy: 0.3222 - val_loss: 1.8313 - val_accuracy: 0.5116\n",
      "Epoch 4/10\n",
      "900/900 [==============================] - 15s 17ms/step - loss: 1.7530 - accuracy: 0.4089 - val_loss: 1.5850 - val_accuracy: 0.4725\n",
      "Epoch 5/10\n",
      "900/900 [==============================] - 12s 13ms/step - loss: 1.4358 - accuracy: 0.5144 - val_loss: 1.3141 - val_accuracy: 0.6562\n",
      "Epoch 6/10\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 1.2634 - accuracy: 0.5767 - val_loss: 1.3984 - val_accuracy: 0.5030\n",
      "Epoch 7/10\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 1.2360 - accuracy: 0.5833 - val_loss: 0.8538 - val_accuracy: 0.7406\n",
      "Epoch 8/10\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 0.9265 - accuracy: 0.6922 - val_loss: 0.7182 - val_accuracy: 0.7610\n",
      "Epoch 9/10\n",
      "900/900 [==============================] - 13s 15ms/step - loss: 0.8379 - accuracy: 0.7411 - val_loss: 0.4640 - val_accuracy: 0.8635\n",
      "Epoch 10/10\n",
      "900/900 [==============================] - 11s 12ms/step - loss: 0.6782 - accuracy: 0.7667 - val_loss: 0.4426 - val_accuracy: 0.8651\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9833333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(1000, 28, 28)\n",
      "Testing with train set size of:  (1000, 28, 28)\n",
      "x_train shape: (1000, 28, 28, 1)\n",
      "1000 train samples\n",
      "59000 test samples\n",
      "Train on 1000 samples, validate on 59000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 2.2613 - accuracy: 0.1410 - val_loss: 2.1464 - val_accuracy: 0.3585\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 2.0430 - accuracy: 0.3060 - val_loss: 1.5540 - val_accuracy: 0.5526\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 1.7320 - accuracy: 0.4110 - val_loss: 1.4262 - val_accuracy: 0.5954\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 1.3067 - accuracy: 0.5660 - val_loss: 0.8959 - val_accuracy: 0.7408\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 14s 14ms/step - loss: 1.1117 - accuracy: 0.6290 - val_loss: 0.6011 - val_accuracy: 0.8192\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.8548 - accuracy: 0.7130 - val_loss: 0.4457 - val_accuracy: 0.8622\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.6544 - accuracy: 0.7880 - val_loss: 0.3308 - val_accuracy: 0.9074\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.5503 - accuracy: 0.8120 - val_loss: 0.3417 - val_accuracy: 0.8958\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.4747 - accuracy: 0.8530 - val_loss: 0.2913 - val_accuracy: 0.9123\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.3956 - accuracy: 0.8720 - val_loss: 0.2783 - val_accuracy: 0.9149\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9791666666666666\n",
      "(60000, 28, 28) (60000,)\n",
      "(1250, 28, 28)\n",
      "Testing with train set size of:  (1250, 28, 28)\n",
      "x_train shape: (1250, 28, 28, 1)\n",
      "1250 train samples\n",
      "58750 test samples\n",
      "Train on 1250 samples, validate on 58750 samples\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 2.2592 - accuracy: 0.1728 - val_loss: 2.1093 - val_accuracy: 0.4477\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 2.0263 - accuracy: 0.2792 - val_loss: 1.5063 - val_accuracy: 0.5375\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.5958 - accuracy: 0.4568 - val_loss: 1.3172 - val_accuracy: 0.6141\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.1585 - accuracy: 0.6368 - val_loss: 0.6477 - val_accuracy: 0.8657\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.9153 - accuracy: 0.7128 - val_loss: 0.4402 - val_accuracy: 0.9015\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.6712 - accuracy: 0.7864 - val_loss: 0.3313 - val_accuracy: 0.9028\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.6167 - accuracy: 0.8096 - val_loss: 0.3466 - val_accuracy: 0.9031\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4525 - accuracy: 0.8736 - val_loss: 0.2356 - val_accuracy: 0.9320\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4126 - accuracy: 0.8744 - val_loss: 0.2538 - val_accuracy: 0.9260\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3648 - accuracy: 0.9048 - val_loss: 0.2020 - val_accuracy: 0.9416\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.975\n",
      "(60000, 28, 28) (60000,)\n",
      "(1500, 28, 28)\n",
      "Testing with train set size of:  (1500, 28, 28)\n",
      "x_train shape: (1500, 28, 28, 1)\n",
      "1500 train samples\n",
      "58500 test samples\n",
      "Train on 1500 samples, validate on 58500 samples\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 2.1906 - accuracy: 0.2193 - val_loss: 1.8545 - val_accuracy: 0.4542\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 1.7587 - accuracy: 0.4093 - val_loss: 1.1351 - val_accuracy: 0.7062\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 1.2275 - accuracy: 0.5827 - val_loss: 0.9144 - val_accuracy: 0.7110\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.8793 - accuracy: 0.7060 - val_loss: 0.5965 - val_accuracy: 0.7951\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.7160 - accuracy: 0.7753 - val_loss: 0.3531 - val_accuracy: 0.8991\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.5515 - accuracy: 0.8340 - val_loss: 0.3380 - val_accuracy: 0.8953\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.5249 - accuracy: 0.8293 - val_loss: 0.2361 - val_accuracy: 0.9311\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 11s 8ms/step - loss: 0.4148 - accuracy: 0.8640 - val_loss: 0.1959 - val_accuracy: 0.9431\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.3457 - accuracy: 0.8967 - val_loss: 0.1820 - val_accuracy: 0.9468\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.3126 - accuracy: 0.9073 - val_loss: 0.1947 - val_accuracy: 0.9437\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9708333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(1750, 28, 28)\n",
      "Testing with train set size of:  (1750, 28, 28)\n",
      "x_train shape: (1750, 28, 28, 1)\n",
      "1750 train samples\n",
      "58250 test samples\n",
      "Train on 1750 samples, validate on 58250 samples\n",
      "Epoch 1/10\n",
      "1750/1750 [==============================] - 13s 8ms/step - loss: 2.1445 - accuracy: 0.2280 - val_loss: 1.8178 - val_accuracy: 0.4917\n",
      "Epoch 2/10\n",
      "1750/1750 [==============================] - 12s 7ms/step - loss: 1.5816 - accuracy: 0.4554 - val_loss: 1.0556 - val_accuracy: 0.7162\n",
      "Epoch 3/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 1.0835 - accuracy: 0.6406 - val_loss: 0.6808 - val_accuracy: 0.8553\n",
      "Epoch 4/10\n",
      "1750/1750 [==============================] - 11s 7ms/step - loss: 0.7415 - accuracy: 0.7520 - val_loss: 0.4008 - val_accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "1750/1750 [==============================] - 12s 7ms/step - loss: 0.5698 - accuracy: 0.8183 - val_loss: 0.2825 - val_accuracy: 0.9226\n",
      "Epoch 6/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 0.4523 - accuracy: 0.8543 - val_loss: 0.2431 - val_accuracy: 0.9306\n",
      "Epoch 7/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 0.4026 - accuracy: 0.8823 - val_loss: 0.2365 - val_accuracy: 0.9317\n",
      "Epoch 8/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 0.3410 - accuracy: 0.9046 - val_loss: 0.1931 - val_accuracy: 0.9449\n",
      "Epoch 9/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 0.2691 - accuracy: 0.9189 - val_loss: 0.2160 - val_accuracy: 0.9352\n",
      "Epoch 10/10\n",
      "1750/1750 [==============================] - 11s 6ms/step - loss: 0.2721 - accuracy: 0.9160 - val_loss: 0.1944 - val_accuracy: 0.9456\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9666666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(2000, 28, 28)\n",
      "Testing with train set size of:  (2000, 28, 28)\n",
      "x_train shape: (2000, 28, 28, 1)\n",
      "2000 train samples\n",
      "58000 test samples\n",
      "Train on 2000 samples, validate on 58000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 2.1956 - accuracy: 0.2075 - val_loss: 1.9066 - val_accuracy: 0.2585\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.6209 - accuracy: 0.4570 - val_loss: 0.8691 - val_accuracy: 0.7522\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 1.0078 - accuracy: 0.6675 - val_loss: 0.4458 - val_accuracy: 0.8822\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.6751 - accuracy: 0.7725 - val_loss: 0.3530 - val_accuracy: 0.8985\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.5387 - accuracy: 0.8360 - val_loss: 0.3306 - val_accuracy: 0.8962\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 11s 6ms/step - loss: 0.4131 - accuracy: 0.8740 - val_loss: 0.2254 - val_accuracy: 0.9338\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3188 - accuracy: 0.8995 - val_loss: 0.1970 - val_accuracy: 0.9372\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 13s 6ms/step - loss: 0.2794 - accuracy: 0.9230 - val_loss: 0.2063 - val_accuracy: 0.9388\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2488 - accuracy: 0.9265 - val_loss: 0.1733 - val_accuracy: 0.9487\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2203 - accuracy: 0.9325 - val_loss: 0.1617 - val_accuracy: 0.9504\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9625\n",
      "(60000, 28, 28) (60000,)\n",
      "(2250, 28, 28)\n",
      "Testing with train set size of:  (2250, 28, 28)\n",
      "x_train shape: (2250, 28, 28, 1)\n",
      "2250 train samples\n",
      "57750 test samples\n",
      "Train on 2250 samples, validate on 57750 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250/2250 [==============================] - 12s 5ms/step - loss: 2.0784 - accuracy: 0.2591 - val_loss: 1.3878 - val_accuracy: 0.6773\n",
      "Epoch 2/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 1.2639 - accuracy: 0.5764 - val_loss: 0.6113 - val_accuracy: 0.8645\n",
      "Epoch 3/10\n",
      "2250/2250 [==============================] - 15s 7ms/step - loss: 0.7735 - accuracy: 0.7462 - val_loss: 0.4079 - val_accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "2250/2250 [==============================] - 13s 6ms/step - loss: 0.5514 - accuracy: 0.8267 - val_loss: 0.3666 - val_accuracy: 0.8918\n",
      "Epoch 5/10\n",
      "2250/2250 [==============================] - 15s 7ms/step - loss: 0.4531 - accuracy: 0.8551 - val_loss: 0.2139 - val_accuracy: 0.9367\n",
      "Epoch 6/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 0.3345 - accuracy: 0.9036 - val_loss: 0.1892 - val_accuracy: 0.9477\n",
      "Epoch 7/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 0.2858 - accuracy: 0.9173 - val_loss: 0.2192 - val_accuracy: 0.9341\n",
      "Epoch 8/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 0.2401 - accuracy: 0.9271 - val_loss: 0.1622 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 0.1986 - accuracy: 0.9444 - val_loss: 0.1767 - val_accuracy: 0.9511\n",
      "Epoch 10/10\n",
      "2250/2250 [==============================] - 12s 5ms/step - loss: 0.1787 - accuracy: 0.9533 - val_loss: 0.1419 - val_accuracy: 0.9594\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9583333333333334\n",
      "(60000, 28, 28) (60000,)\n",
      "(2500, 28, 28)\n",
      "Testing with train set size of:  (2500, 28, 28)\n",
      "x_train shape: (2500, 28, 28, 1)\n",
      "2500 train samples\n",
      "57500 test samples\n",
      "Train on 2500 samples, validate on 57500 samples\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 2.0904 - accuracy: 0.2496 - val_loss: 1.5259 - val_accuracy: 0.5297\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 1.2405 - accuracy: 0.5864 - val_loss: 0.5859 - val_accuracy: 0.8416\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.6781 - accuracy: 0.7816 - val_loss: 0.4614 - val_accuracy: 0.8475\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.5001 - accuracy: 0.8464 - val_loss: 0.2488 - val_accuracy: 0.9262\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.3420 - accuracy: 0.8928 - val_loss: 0.1832 - val_accuracy: 0.9468\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.2895 - accuracy: 0.9196 - val_loss: 0.1831 - val_accuracy: 0.9459\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.2608 - accuracy: 0.9228 - val_loss: 0.2180 - val_accuracy: 0.9382\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.2121 - accuracy: 0.9340 - val_loss: 0.1697 - val_accuracy: 0.9498\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.1756 - accuracy: 0.9508 - val_loss: 0.1378 - val_accuracy: 0.9617\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 14s 6ms/step - loss: 0.1527 - accuracy: 0.9516 - val_loss: 0.1642 - val_accuracy: 0.9551\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9541666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(2750, 28, 28)\n",
      "Testing with train set size of:  (2750, 28, 28)\n",
      "x_train shape: (2750, 28, 28, 1)\n",
      "2750 train samples\n",
      "57250 test samples\n",
      "Train on 2750 samples, validate on 57250 samples\n",
      "Epoch 1/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 2.0159 - accuracy: 0.2909 - val_loss: 1.3250 - val_accuracy: 0.6320\n",
      "Epoch 2/10\n",
      "2750/2750 [==============================] - 12s 5ms/step - loss: 1.2154 - accuracy: 0.5898 - val_loss: 0.6223 - val_accuracy: 0.7780\n",
      "Epoch 3/10\n",
      "2750/2750 [==============================] - 14s 5ms/step - loss: 0.7296 - accuracy: 0.7611 - val_loss: 0.3515 - val_accuracy: 0.8979\n",
      "Epoch 4/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 0.4530 - accuracy: 0.8553 - val_loss: 0.3681 - val_accuracy: 0.8848\n",
      "Epoch 5/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 0.3636 - accuracy: 0.8920 - val_loss: 0.1918 - val_accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 0.2782 - accuracy: 0.9182 - val_loss: 0.2141 - val_accuracy: 0.9344\n",
      "Epoch 7/10\n",
      "2750/2750 [==============================] - 14s 5ms/step - loss: 0.2220 - accuracy: 0.9342 - val_loss: 0.1676 - val_accuracy: 0.9499\n",
      "Epoch 8/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 0.1849 - accuracy: 0.9524 - val_loss: 0.1383 - val_accuracy: 0.9597\n",
      "Epoch 9/10\n",
      "2750/2750 [==============================] - 12s 5ms/step - loss: 0.1731 - accuracy: 0.9498 - val_loss: 0.1360 - val_accuracy: 0.9624\n",
      "Epoch 10/10\n",
      "2750/2750 [==============================] - 13s 5ms/step - loss: 0.1421 - accuracy: 0.9589 - val_loss: 0.1401 - val_accuracy: 0.9602\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.95\n",
      "(60000, 28, 28) (60000,)\n",
      "(3000, 28, 28)\n",
      "Testing with train set size of:  (3000, 28, 28)\n",
      "x_train shape: (3000, 28, 28, 1)\n",
      "3000 train samples\n",
      "57000 test samples\n",
      "Train on 3000 samples, validate on 57000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 2.0605 - accuracy: 0.2633 - val_loss: 1.3640 - val_accuracy: 0.5233\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 1.0907 - accuracy: 0.6403 - val_loss: 0.3936 - val_accuracy: 0.8988\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 0.5858 - accuracy: 0.8187 - val_loss: 0.2448 - val_accuracy: 0.9302\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.4116 - accuracy: 0.8747 - val_loss: 0.1936 - val_accuracy: 0.9438\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.3292 - accuracy: 0.9053 - val_loss: 0.1569 - val_accuracy: 0.9537\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.2438 - accuracy: 0.9323 - val_loss: 0.1289 - val_accuracy: 0.9617\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 0.2155 - accuracy: 0.9397 - val_loss: 0.1342 - val_accuracy: 0.9601\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.2070 - accuracy: 0.9440 - val_loss: 0.1304 - val_accuracy: 0.9625\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.1605 - accuracy: 0.9577 - val_loss: 0.1507 - val_accuracy: 0.9569\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.1696 - accuracy: 0.9520 - val_loss: 0.1177 - val_accuracy: 0.9662\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9458333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(3250, 28, 28)\n",
      "Testing with train set size of:  (3250, 28, 28)\n",
      "x_train shape: (3250, 28, 28, 1)\n",
      "3250 train samples\n",
      "56750 test samples\n",
      "Train on 3250 samples, validate on 56750 samples\n",
      "Epoch 1/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 1.9252 - accuracy: 0.3169 - val_loss: 0.9690 - val_accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.9199 - accuracy: 0.6849 - val_loss: 0.4239 - val_accuracy: 0.8675\n",
      "Epoch 3/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.5430 - accuracy: 0.8283 - val_loss: 0.2258 - val_accuracy: 0.9342\n",
      "Epoch 4/10\n",
      "3250/3250 [==============================] - 14s 4ms/step - loss: 0.3588 - accuracy: 0.8948 - val_loss: 0.2517 - val_accuracy: 0.9245\n",
      "Epoch 5/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.2946 - accuracy: 0.9135 - val_loss: 0.1521 - val_accuracy: 0.9559\n",
      "Epoch 6/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.2352 - accuracy: 0.9308 - val_loss: 0.1474 - val_accuracy: 0.9570\n",
      "Epoch 7/10\n",
      "3250/3250 [==============================] - 15s 5ms/step - loss: 0.2024 - accuracy: 0.9425 - val_loss: 0.1267 - val_accuracy: 0.9639\n",
      "Epoch 8/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.1763 - accuracy: 0.9526 - val_loss: 0.1253 - val_accuracy: 0.9647\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.1485 - accuracy: 0.9594 - val_loss: 0.1255 - val_accuracy: 0.9651\n",
      "Epoch 10/10\n",
      "3250/3250 [==============================] - 13s 4ms/step - loss: 0.1437 - accuracy: 0.9594 - val_loss: 0.1175 - val_accuracy: 0.9665\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9416666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(3500, 28, 28)\n",
      "Testing with train set size of:  (3500, 28, 28)\n",
      "x_train shape: (3500, 28, 28, 1)\n",
      "3500 train samples\n",
      "56500 test samples\n",
      "Train on 3500 samples, validate on 56500 samples\n",
      "Epoch 1/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 1.8775 - accuracy: 0.3454 - val_loss: 0.9127 - val_accuracy: 0.7458\n",
      "Epoch 2/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.8391 - accuracy: 0.7206 - val_loss: 0.3837 - val_accuracy: 0.8894\n",
      "Epoch 3/10\n",
      "3500/3500 [==============================] - 15s 4ms/step - loss: 0.4966 - accuracy: 0.8563 - val_loss: 0.2199 - val_accuracy: 0.9379\n",
      "Epoch 4/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.3393 - accuracy: 0.9020 - val_loss: 0.1755 - val_accuracy: 0.9485\n",
      "Epoch 5/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.2732 - accuracy: 0.9154 - val_loss: 0.1834 - val_accuracy: 0.9467\n",
      "Epoch 6/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.2387 - accuracy: 0.9311 - val_loss: 0.1491 - val_accuracy: 0.9550\n",
      "Epoch 7/10\n",
      "3500/3500 [==============================] - 15s 4ms/step - loss: 0.2082 - accuracy: 0.9431 - val_loss: 0.1465 - val_accuracy: 0.9561\n",
      "Epoch 8/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.1809 - accuracy: 0.9523 - val_loss: 0.1613 - val_accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.1580 - accuracy: 0.9537 - val_loss: 0.1154 - val_accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "3500/3500 [==============================] - 13s 4ms/step - loss: 0.1295 - accuracy: 0.9620 - val_loss: 0.1309 - val_accuracy: 0.9642\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9375\n",
      "(60000, 28, 28) (60000,)\n",
      "(3750, 28, 28)\n",
      "Testing with train set size of:  (3750, 28, 28)\n",
      "x_train shape: (3750, 28, 28, 1)\n",
      "3750 train samples\n",
      "56250 test samples\n",
      "Train on 3750 samples, validate on 56250 samples\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.8524 - accuracy: 0.3520 - val_loss: 0.8080 - val_accuracy: 0.7885\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.7989 - accuracy: 0.7381 - val_loss: 0.3112 - val_accuracy: 0.9142\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 0.4518 - accuracy: 0.8675 - val_loss: 0.2962 - val_accuracy: 0.9212\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.3169 - accuracy: 0.9045 - val_loss: 0.1884 - val_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.2485 - accuracy: 0.9312 - val_loss: 0.1679 - val_accuracy: 0.9521\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.2333 - accuracy: 0.9328 - val_loss: 0.1643 - val_accuracy: 0.9533\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 0.1865 - accuracy: 0.9456 - val_loss: 0.1217 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.1499 - accuracy: 0.9544 - val_loss: 0.1115 - val_accuracy: 0.9674\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 0.1380 - accuracy: 0.9581 - val_loss: 0.1078 - val_accuracy: 0.9686\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 0.1231 - accuracy: 0.9627 - val_loss: 0.1088 - val_accuracy: 0.9686\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9333333333333333\n",
      "(60000, 28, 28) (60000,)\n",
      "(4000, 28, 28)\n",
      "Testing with train set size of:  (4000, 28, 28)\n",
      "x_train shape: (4000, 28, 28, 1)\n",
      "4000 train samples\n",
      "56000 test samples\n",
      "Train on 4000 samples, validate on 56000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 1.8822 - accuracy: 0.3277 - val_loss: 0.9661 - val_accuracy: 0.7822\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.8389 - accuracy: 0.7297 - val_loss: 0.4795 - val_accuracy: 0.8451\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 15s 4ms/step - loss: 0.4317 - accuracy: 0.8685 - val_loss: 0.3064 - val_accuracy: 0.9028\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.3220 - accuracy: 0.9028 - val_loss: 0.1766 - val_accuracy: 0.9444\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.2371 - accuracy: 0.9277 - val_loss: 0.1655 - val_accuracy: 0.9486\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.2168 - accuracy: 0.9383 - val_loss: 0.1603 - val_accuracy: 0.9526\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 15s 4ms/step - loss: 0.1896 - accuracy: 0.9442 - val_loss: 0.1128 - val_accuracy: 0.9672\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.1604 - accuracy: 0.9550 - val_loss: 0.1494 - val_accuracy: 0.9560\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 14s 3ms/step - loss: 0.1468 - accuracy: 0.9575 - val_loss: 0.1056 - val_accuracy: 0.9705\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 15s 4ms/step - loss: 0.1318 - accuracy: 0.9613 - val_loss: 0.1089 - val_accuracy: 0.9689\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9291666666666667\n",
      "(60000, 28, 28) (60000,)\n",
      "(4250, 28, 28)\n",
      "Testing with train set size of:  (4250, 28, 28)\n",
      "x_train shape: (4250, 28, 28, 1)\n",
      "4250 train samples\n",
      "55750 test samples\n",
      "Train on 4250 samples, validate on 55750 samples\n",
      "Epoch 1/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 1.8046 - accuracy: 0.3800 - val_loss: 0.9105 - val_accuracy: 0.6897\n",
      "Epoch 2/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 0.7555 - accuracy: 0.7433 - val_loss: 0.3631 - val_accuracy: 0.8833\n",
      "Epoch 3/10\n",
      "4250/4250 [==============================] - 16s 4ms/step - loss: 0.4446 - accuracy: 0.8605 - val_loss: 0.1897 - val_accuracy: 0.9443\n",
      "Epoch 4/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 0.3066 - accuracy: 0.9108 - val_loss: 0.2721 - val_accuracy: 0.9186\n",
      "Epoch 5/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 0.2480 - accuracy: 0.9271 - val_loss: 0.1407 - val_accuracy: 0.9596\n",
      "Epoch 6/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 0.1974 - accuracy: 0.9435 - val_loss: 0.1570 - val_accuracy: 0.9545\n",
      "Epoch 7/10\n",
      "4250/4250 [==============================] - 15s 4ms/step - loss: 0.1712 - accuracy: 0.9480 - val_loss: 0.1651 - val_accuracy: 0.9546\n",
      "Epoch 8/10\n",
      "4250/4250 [==============================] - 13s 3ms/step - loss: 0.1525 - accuracy: 0.9555 - val_loss: 0.1085 - val_accuracy: 0.9691\n",
      "Epoch 9/10\n",
      "4250/4250 [==============================] - 14s 3ms/step - loss: 0.1338 - accuracy: 0.9621 - val_loss: 0.1207 - val_accuracy: 0.9660\n",
      "Epoch 10/10\n",
      "4250/4250 [==============================] - 16s 4ms/step - loss: 0.1292 - accuracy: 0.9642 - val_loss: 0.1087 - val_accuracy: 0.9707\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.925\n",
      "(60000, 28, 28) (60000,)\n",
      "(4500, 28, 28)\n",
      "Testing with train set size of:  (4500, 28, 28)\n",
      "x_train shape: (4500, 28, 28, 1)\n",
      "4500 train samples\n",
      "55500 test samples\n",
      "Train on 4500 samples, validate on 55500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 1.8119 - accuracy: 0.3649 - val_loss: 0.9295 - val_accuracy: 0.7055\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 0.7687 - accuracy: 0.7507 - val_loss: 0.3274 - val_accuracy: 0.9008\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 16s 3ms/step - loss: 0.4379 - accuracy: 0.8678 - val_loss: 0.1899 - val_accuracy: 0.9449\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 0.3295 - accuracy: 0.9013 - val_loss: 0.2858 - val_accuracy: 0.9091\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 0.2615 - accuracy: 0.9300 - val_loss: 0.1331 - val_accuracy: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 0.1834 - accuracy: 0.9456 - val_loss: 0.1262 - val_accuracy: 0.9627\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.1727 - accuracy: 0.9500 - val_loss: 0.1284 - val_accuracy: 0.9615\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 14s 3ms/step - loss: 0.1571 - accuracy: 0.9540 - val_loss: 0.2150 - val_accuracy: 0.9403\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 13s 3ms/step - loss: 0.1340 - accuracy: 0.9638 - val_loss: 0.1028 - val_accuracy: 0.9721\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.1184 - accuracy: 0.9649 - val_loss: 0.1144 - val_accuracy: 0.9668\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9208333333333334\n",
      "(60000, 28, 28) (60000,)\n",
      "(4750, 28, 28)\n",
      "Testing with train set size of:  (4750, 28, 28)\n",
      "x_train shape: (4750, 28, 28, 1)\n",
      "4750 train samples\n",
      "55250 test samples\n",
      "Train on 4750 samples, validate on 55250 samples\n",
      "Epoch 1/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 1.7100 - accuracy: 0.3994 - val_loss: 0.6681 - val_accuracy: 0.7893\n",
      "Epoch 2/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.6823 - accuracy: 0.7827 - val_loss: 0.5081 - val_accuracy: 0.8489\n",
      "Epoch 3/10\n",
      "4750/4750 [==============================] - 15s 3ms/step - loss: 0.4002 - accuracy: 0.8872 - val_loss: 0.1572 - val_accuracy: 0.9546\n",
      "Epoch 4/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.2640 - accuracy: 0.9280 - val_loss: 0.1512 - val_accuracy: 0.9573\n",
      "Epoch 5/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.2310 - accuracy: 0.9373 - val_loss: 0.1166 - val_accuracy: 0.9662\n",
      "Epoch 6/10\n",
      "4750/4750 [==============================] - 16s 3ms/step - loss: 0.1859 - accuracy: 0.9482 - val_loss: 0.1102 - val_accuracy: 0.9674\n",
      "Epoch 7/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.1730 - accuracy: 0.9516 - val_loss: 0.1063 - val_accuracy: 0.9692\n",
      "Epoch 8/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.1377 - accuracy: 0.9596 - val_loss: 0.1049 - val_accuracy: 0.9692\n",
      "Epoch 9/10\n",
      "4750/4750 [==============================] - 14s 3ms/step - loss: 0.1190 - accuracy: 0.9657 - val_loss: 0.0934 - val_accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "4750/4750 [==============================] - 16s 3ms/step - loss: 0.1072 - accuracy: 0.9682 - val_loss: 0.2269 - val_accuracy: 0.9423\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "0.9166666666666666\n",
      "(60000, 28, 28) (60000,)\n",
      "(5000, 28, 28)\n",
      "Testing with train set size of:  (5000, 28, 28)\n",
      "x_train shape: (5000, 28, 28, 1)\n",
      "5000 train samples\n",
      "55000 test samples\n",
      "Train on 5000 samples, validate on 55000 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 1.8116 - accuracy: 0.3558 - val_loss: 1.3767 - val_accuracy: 0.4889\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.7656 - accuracy: 0.7548 - val_loss: 0.3476 - val_accuracy: 0.9034\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.4259 - accuracy: 0.8680 - val_loss: 0.2524 - val_accuracy: 0.9263\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.3068 - accuracy: 0.9048 - val_loss: 0.2304 - val_accuracy: 0.9275\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.2258 - accuracy: 0.9340 - val_loss: 0.1714 - val_accuracy: 0.9468\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.1964 - accuracy: 0.9368 - val_loss: 0.1280 - val_accuracy: 0.9645\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1823 - accuracy: 0.9476 - val_loss: 0.1065 - val_accuracy: 0.9692\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1412 - accuracy: 0.9598 - val_loss: 0.1077 - val_accuracy: 0.9692\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.1297 - accuracy: 0.9602 - val_loss: 0.1233 - val_accuracy: 0.9650\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1135 - accuracy: 0.9682 - val_loss: 0.0984 - val_accuracy: 0.9728\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n",
      "{(100, 28, 28, 1): {0.41360601782798767, 1.0, 1.9473908741366683}, (200, 28, 28, 1): {0.5123578310012817, 1.7485677863124223, 2.0}, (300, 28, 28, 1): {0.9859188210025704, 0.7883082032203674, 3.0}, (400, 28, 28, 1): {0.8299333902173395, 0.7592952847480774, 4.0}, (500, 28, 28, 1): {0.6112979846501551, 0.8488907814025879, 5.0}, (600, 28, 28, 1): {0.3970352809035818, 0.9013131260871887, 6.0}, (700, 28, 28, 1): {0.31940235241569115, 0.903642475605011, 7.0}, (800, 28, 28, 1): {8.0, 0.3717542235915725, 0.8868749737739563}, (900, 28, 28, 1): {0.44257667893685665, 9.0, 0.8651099801063538}, (1000, 28, 28, 1): {0.2782689767591024, 0.9149321913719177, 10.0}, (1250, 28, 28, 1): {0.20204347675911924, 0.9416170120239258, 12.5}, (1500, 28, 28, 1): {0.1946935737489635, 0.9437435865402222, 15.0}, (1750, 28, 28, 1): {0.1944182147186873, 17.5, 0.9456309080123901}, (2000, 28, 28, 1): {0.1616910392809017, 0.9504482746124268, 20.0}, (2250, 28, 28, 1): {0.14191391077379645, 0.9594285488128662, 22.5}, (2500, 28, 28, 1): {0.16420171630298314, 25.0, 0.9550782442092896}, (2750, 28, 28, 1): {0.14014463707147418, 0.9602096080780029, 27.5}, (3000, 28, 28, 1): {0.11771224454435798, 0.9662280678749084, 30.0}, (3250, 28, 28, 1): {32.5, 0.966519832611084, 0.11747534622235577}, (3500, 28, 28, 1): {0.13085077676460544, 0.9642124176025391, 35.0}, (3750, 28, 28, 1): {0.10876199407554335, 0.9685510993003845, 37.5}, (4000, 28, 28, 1): {40.0, 0.10887770517852291, 0.9689106941223145}, (4250, 28, 28, 1): {0.10870982873496796, 0.9707264304161072, 42.5}, (4500, 28, 28, 1): {0.11442138385119101, 0.9667567610740662, 45.0}, (4750, 28, 28, 1): {0.22694782386888745, 0.9423348307609558, 47.5}, (5000, 28, 28, 1): {0.09841099160335619, 0.9728000164031982, 50.0}}\n"
     ]
    }
   ],
   "source": [
    "summary_values = {}\n",
    "\n",
    "# Takes increasingly more images to increase the accuracy\n",
    "training_sets = [100,200 ,300 ,400 , 500, 600, 700, 800, 900, 1000, 1250,1500,1750,2000,2250,2500,2750,3000,3250,3500,3750,4000,4250,4500,4750,5000]\n",
    "j = 0\n",
    "column_names = [\"Amount of Training images\", \"Accuracy\"]\n",
    "df = pd.DataFrame(columns = column_names, index = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "\n",
    "for test_per in training_sets:\n",
    "    training_per = 1- (test_per/60000) #calculation is giving the test percentage\n",
    "    print(training_per)\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data(training_per)\n",
    "    print('Testing with train set size of: ', x_train.shape)\n",
    "    x_train, y_train, x_test, y_test = prepare_input(x_train, y_train, x_test, y_test)\n",
    "    score = setup_model(x_train, y_train, x_test, y_test)\n",
    "    summary_values[x_train.shape] = {test_per/100, score[0], score[1]}\n",
    "    i = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "    new_data_list = [test_per, score[1]]\n",
    "    df.loc[i[j]] = new_data_list\n",
    "    j += 1\n",
    "\n",
    "\n",
    "print(summary_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final step for this project and does not involve much. The three main things that are part of this step are the variables summary_values, training_sets and the for loop. The variable summary_values is a dictionary variable that allows for the ending data for each training size to be collected and stored safely. This variable is printed at the end to allow for easy data collection. The variable training_sets is used in conjunction with the for loop to iterate through each amount of training images. The variable test_per is being iterated with each value in the array training_sets and is converted into a percentage to be put into the function load_mnist_data and specify what percent of the images are training. The results of the project are displayed above and the data table and graphs are made and printed down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Amount of Training images  Accuracy\n",
      "1                        100  0.413606\n",
      "2                        200  0.512358\n",
      "3                        300  0.788308\n",
      "4                        400  0.759295\n",
      "5                        500  0.848891\n",
      "6                        600  0.901313\n",
      "7                        700  0.903642\n",
      "8                        800  0.886875\n",
      "9                        900   0.86511\n",
      "10                      1000  0.914932\n",
      "11                      1250  0.941617\n",
      "12                      1500  0.943744\n",
      "13                      1750  0.945631\n",
      "14                      2000  0.950448\n",
      "15                      2250  0.959429\n",
      "16                      2500  0.955078\n",
      "17                      2750   0.96021\n",
      "18                      3000  0.966228\n",
      "19                      3250   0.96652\n",
      "20                      3500  0.964212\n",
      "21                      3750  0.968551\n",
      "22                      4000  0.968911\n",
      "23                      4250  0.970726\n",
      "24                      4500  0.966757\n",
      "25                      4750  0.942335\n",
      "26                      5000    0.9728\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Line Graph\n",
    "line_graph = df.plot(kind='line',x='Amount of Training images',y='Accuracy',color='red',  xticks=[100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,5000], \n",
    "        yticks=[0.5,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1.00], figsize=(25,10))\n",
    "line_graph.set(title=\"How changing the amount of Training data impacts the accuracy\", xlabel=\"Amount of training images\", ylabel=\"Accuracy in decimal(%)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was a very interesting project overall. The big takeaways from this project were that the more training data that a machine learning algorithm is provided with, the higher the accuracy is. Machine learning algorithms are also highly sensitive to patterns and at any moment a slight pattern emerges, the algorithm shifts to accomodate that pattern. The hypothesis was also proven right with the results showing a drastic, and highly noticeable difference between the first training set of 100, and the last training set of 50,000, which got to 99% accuracy. All of this serves to quantify the notion that machines need a lot of examples and time to to learn/understand what they are looking at, and apply that to test images. The first training sets had very low accuracy, and this was because it was being tested on something it might not even have seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is just a stepping stone into the world of machine leearning. Machine learning and artificial intelligence have huge applications in the future, allowing for humans to take a backseat in  the world and allow computers to do more and more things for us. Machine learning works extremely similarly to a human brain. Neural networks are just very, very small simulations of actual neurons in the brain. This project used just one kind of neural network, a convolutional neural network. However, there are a lot of differing kinds of neural networks that allow for increasingly complicated and better implementations of artificial intelligence for purposes as varied as facial recognition and YouTube ads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link for the same project but in Visual studio code for ease of copying and pasting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Harshith0307/Machine_Learning/blob/master/minst_handwriting.py\n",
    "\n",
    "You can copy and paste this into Visual Studio Code, and if all the packages are installed, it will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bibliography for all of the information used in this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical analysis. (2020). In Encyclopdia Britannica. Retrieved from https://school.eb.com/levels/high/article/numerical-analysis/108506#\n",
    "\n",
    "Published in Encyclopedia Britannica, this article discusses numerical analysis and all its applications. The main focus of this article is numerical analysis use in computer science and data science applications. This details numerical regressions and algorithms that are regularly used to find correlations within data sets and serve as a basis for almost all machine learning algorithms.\n",
    "\n",
    "Encyclopedia Britannica, the publisher of the article, is a British encyclopedia publisher that has long been renowned for having extremely neutral and unbiased content. Britannica has been publishing since 1768 and is a reliable source of information for research purposes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Understanding How Python is Used in Data Science. (n.d.). Retrieved January 25, 2020, from https://www.datasciencegraduateprograms.com/python/ \n",
    "\n",
    "Published on Data Science Graduate Programs, this article discusses the uses of Python within the realm of data science. Python is an object-oriented programming language that is used in data science for its versatility, and ease of operating with additional libraries for an overall very simple approach to data science and machine learning. This article outlines the uses of Python in Data Science and how python efficiently and effectively fulfills that goal.\n",
    "\n",
    "Data Science Graduate Programs is a California based website staffed by professors in California. It serves as a website to get high school graduates into college for data science and outlines the positives of the data science career. It is a reliable and unbiased source and has been publishing since 2011.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Brownlee, Jason. (2019, August 9). Introduction to Matrices and Matrix Arithmetic for Machine Learning. Retrieved February 2, 2020, from https://machinelearningmastery.com/introduction-matrices-machine-learning/\n",
    "\n",
    "Matrices are a fundamental element in linear algebra. They are used widely in the realm of machine learning especially in the input data variable(x) to train the algorithm. A matrix is at its lowest complexity, an array within an array with columns and rows. Matrices can be used in arithmetic operations as well and have been made greatly easier by python libraries such as pandas, and NumPy.\n",
    "\n",
    "Jason Brownlee Ph.D. is a professional python developer and machine learning practitioner. He started writing a series of articles about machine learning with python because he wants to get new developers started in machine learning and get them good fast. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "General Python FAQ. (n.d.). Retrieved January 25, 2020, from https://docs.python.org/2/faq/general.html#what-is-python \n",
    "\n",
    "Python is an object-oriented language. It is a high-level programming language that incorporates a large number of modules, libraries, classes and has a lot of the features that make other programming languages like C, C#, and Java popular. Python is also a portable programming language like Java and can be used on almost all kinds of software.\n",
    "\n",
    "Python.org is a nonprofit website created to house the python documentation, its source code, and other frequently asked questions about python. It also has sections written by and sections with interviews with the original creator of python, Guido Van Rossum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataman. (2019, November 20). What Is Image Recognition? Retrieved February 2, 2020, from https://towardsdatascience.com/module-6-image-recognition-for-insurance-claim-handling-part-i-a338d16c9de0\n",
    "\n",
    "Image recognition is exactly what it sounds like. It is the recognition of images. The human brain makes this really easy, but computers, on the other hand, dont have the intuition that humans have to recognize and evaluate images. There are 4 steps that are required to teach computers how to recognize images. First, all images are made up of pixels and you need to extract them from the image. Secondly, classify those pixels into labeled data sets. Thirdly, the hardest step, train the machine to be able to recognize images. lastly, Predict a new image that classifies under one of the categories. \n",
    "\n",
    "Towards Data Science us an online journal about machine learning, and artificial intelligence written by a variety of authors who are experts in the topic. They strive to provide well written and informative articles that their audience wants to read. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "About pandas. (n.d.). Retrieved February 2, 2020, from https://pandas.pydata.org/about/index.html\n",
    "\n",
    "Pandas is a language library written for the Python programming language. It is mainly a data science library with many applications for machine learning and artificial intelligence. Pandas mainly deals with matrices and large data sets. Pandas is also open source so any developer can suit it to his or her needs.\n",
    "\n",
    "Pandas aims to be the fundamental high-level building block for doing practical data analysis in Python. Additionally, it has the broader goal of becoming a powerful and flexible open-source data analysis/manipulation tool available in any language.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Saha, S. (2018, December 17). A Comprehensive Guide to Convolutional Neural Networks-the ELI5 way. Retrieved March 17, 2020, from https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "The image of the convolutional neural network at the top of the article is used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Yal, O. G. (2020, March 4). Image Classification in 10 Minutes with MNIST Dataset. Retrieved March 17, 2020, from https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\n",
    "\n",
    "An image of the mnist database from the article is used to illustrate how the database's images look\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bhattarai, S. (2018, September 4). What is Gradient Descent in machine learning? Retrieved March 17, 2020, from https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/\n",
    "\n",
    "An image of gradiel descent is used from the article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
